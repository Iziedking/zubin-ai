# Test profile - Full knob surface for ROMA-DSPy runs

project: roma-dspy
version: "0.1.0"
environment: development

agents:
  atomizer:
    llm:
      model: "openrouter/google/gemini-2.5-flash-lite-preview-09-2025"
      temperature: 0.7
      max_tokens: 128000
      timeout: 30
      api_key: null
      base_url: null
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"
    toolkits: []          # See example under agent_mapping if you want task-specific tools
    enabled: true
    type: null            # Options: ATOMIZER, PLANNER, EXECUTOR, AGGREGATOR, VERIFIER
    task_type: null       # Options: RETRIEVE, WRITE, THINK, CODE_INTERPRET, IMAGE_GENERATION
    signature: null       # Inline signature (optional), e.g., "goal -> is_atomic: bool, node_type: NodeType"
    signature_instructions: null
    agent_config: {}      # Business-logic params for the agent
    strategy_config: {}   # Params for the prediction strategy

  planner:
    llm:
      model: "openrouter/google/gemini-2.5-flash-preview-09-2025"
      temperature: 0.4
      max_tokens: 128000
      timeout: 30
      api_key: null
      base_url: null
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: true
    type: null
    task_type: null
    signature: null
    signature_instructions: null
    agent_config:
      max_subtasks: 15
    strategy_config: {}

  executor:
    llm:
      model: "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct-0905"
      temperature: 0.75
      max_tokens: 128000
      timeout: 30
      api_key: null
      base_url: null
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"   # Use "react" or "code_act" if you need tools
    toolkits: []                              # e.g., [{class_name: "CalculatorToolkit"}]
    enabled: true
    type: null
    task_type: null
    signature: null
    signature_instructions: null
    agent_config: {}
    strategy_config: {}

  aggregator:
    llm:
      model: "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct-0905"
      temperature: 0.75
      max_tokens: 128000
      timeout: 30
      api_key: null
      base_url: null
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: true
    type: null
    task_type: null
    signature: null
    signature_instructions: null
    agent_config: {}
    strategy_config: {}

  verifier:
    llm:
      model: "gpt-4o-mini"
      temperature: 0.1
      max_tokens: 2000
      timeout: 30
      api_key: null
      base_url: null
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: false
    type: null
    task_type: null
    signature: null
    signature_instructions: null
    agent_config: {}
    strategy_config: {}

agent_mapping:
  executors:
    RETRIEVE:
      llm:
        model: "openrouter/openai/gpt-5-mini:online"
        temperature: 1.0 #DO NOT CHANGE FOR OPENAI
        max_tokens: 128000
        timeout: 500
        num_retries: 3
        cache: true
      prediction_strategy: "chain_of_thought"   # or "react"/"code_act" if you want tools
      toolkits: []                               # add toolkits if using ReAct/CodeAct
      enabled: true
      agent_config: {}                           # e.g., max_executions: 6
      strategy_config: {}

    THINK:
      llm:
        model: "cerebras/gpt-oss-120b"
        temperature: 0.7
        max_tokens: 128000
        timeout: 30
        num_retries: 3
        cache: true
      prediction_strategy: "chain_of_thought"
      toolkits: []
      enabled: true
      agent_config: {}
      strategy_config: {}

    WRITE:
      llm:
        model: "cerebras/gpt-oss-120b"
        temperature: 0.7
        max_tokens: 128000
        timeout: 30
        num_retries: 3
        cache: true
      prediction_strategy: "chain_of_thought"
      toolkits: []
      enabled: true
      agent_config: {}
      strategy_config: {}

  # Required: keep an enabled default executor as fallback and to satisfy registry
  default_executor:
    llm:
      model: "fireworks_ai/accounts/fireworks/models/gpt-oss-120b"
      temperature: 0.7
      max_tokens: 128000
      timeout: 30
      num_retries: 3
      cache: true
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: true
    agent_config: {}
    strategy_config: {}

  # # Defaults for the other required agent types (can be minimal; still must be enabled)
  # default_atomizer:
  #   llm:
  #     model: "openrouter/google/gemini-2.5-flash-lite-preview-09-2025"
  #     temperature: 0.4
  #     max_tokens: 64000
  #     timeout: 30
  #     num_retries: 3
  #     cache: true
  #   prediction_strategy: "chain_of_thought"
  #   toolkits: []
  #   enabled: true
  #   agent_config: {}

  # default_planner:
  #   llm:
  #     model: "openrouter/google/gemini-2.5-flash"
  #     temperature: 0.4
  #     max_tokens: 128000
  #     timeout: 30
  #     num_retries: 3
  #     cache: true
  #   prediction_strategy: "chain_of_thought"
  #   toolkits: []
  #   enabled: true
  #   agent_config:
  #     max_subtasks: 15

  # default_aggregator:
  #   llm:
  #     model: "cerebras/gpt-oss-120b"
  #     temperature: 0.6
  #     max_tokens: 128000
  #     timeout: 30
  #     num_retries: 3
  #     cache: true
  #   prediction_strategy: "chain_of_thought"
  #   toolkits: []
  #   enabled: true
  #   agent_config: {}

runtime:
  max_concurrency: 8
  timeout: 30
  verbose: false
  max_depth: 1
  enable_logging: false
  log_level: DEBUG

  cache:
    enabled: true
    enable_disk_cache: true
    enable_memory_cache: true
    disk_cache_dir: .cache/dspy
    disk_size_limit_bytes: 30000000000
    memory_max_entries: 1000000

resilience:
  retry_strategy: "exponential_backoff"   # fixed_delay | linear_backoff
  max_retries: 3
  base_delay: 1.0
  max_delay: 60.0
  jitter_factor: 0.1
  failure_threshold: 5
  recovery_timeout: 60.0
  success_threshold: 2
  evaluation_window: 300.0

  checkpoint:
    enabled: false
    storage_path: .checkpoints
    auto_checkpoint_triggers:
      - before_planning
      - before_aggregation
    max_checkpoints: 10
    max_age_hours: 24.0
    cleanup_interval_minutes: 60
    default_recovery_strategy: partial     # partial | full | selective
    preserve_partial_results: true
    compress_checkpoints: true
    verify_integrity: true
    periodic_checkpoints_enabled: false
    periodic_interval_seconds: 30.0
    min_execution_time_for_periodic: 10.0

storage:
  base_path: .tmp/sentient
  max_file_size: 104857600
  buffer_size: 1048576
  postgres:
    enabled: false
    connection_url: postgresql+asyncpg://localhost/roma_dspy
    pool_size: 5
    max_overflow: 10
    pool_timeout: 30.0
    echo_sql: false

observability:
  mlflow:
    enabled: false
    tracking_uri: http://127.0.0.1:5000
    experiment_name: ROMA-DSPy
    log_traces: true
    log_traces_from_compile: false
    log_traces_from_eval: true
    log_compiles: true
    log_evals: true
    backend_store_uri: null
    artifact_location: null
  toolkit_metrics:
    enabled: true
    track_lifecycle: true
    track_invocations: true
    sample_rate: 1.0
    persist_to_db: true
    persist_to_mlflow: false
    batch_size: 100
    async_persist: true
  event_traces:
    enabled: true
    track_execution_events: true
    track_module_events: true
    track_task_lifecycle: true
    track_failures: true
    sample_rate: 1.0
    persist_to_db: true
    persist_to_mlflow: false
    batch_size: 50
    async_persist: true
    include_task_details: true
    include_timing: true
    max_goal_length: 200

logging:
  level: ERROR
  log_dir: logs/test/
  console_format: default      # default | minimal | detailed
  file_format: detailed        # default | detailed | json
  colorize: true
  serialize: false
  rotation: "100 MB"
  retention: "30 days"
  compression: "zip"
  intercept_standard_logging: true
  backtrace: true
  diagnose: false
  enqueue: true